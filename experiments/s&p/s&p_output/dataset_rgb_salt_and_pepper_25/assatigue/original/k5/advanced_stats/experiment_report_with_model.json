{
  "experiment_name": "advanced_stats_assatigue_k5_WithModel",
  "config": {
    "dataset_path": "/home/brusc/Projects/random_forest/datasets/dataset_rgb_salt_and_pepper_25/original",
    "area_name": "assatigue",
    "feature_method": "advanced_stats",
    "k_features": 5,
    "output_dir": "/home/brusc/Projects/random_forest/experiments_organized_2/salt_and_pepper_output/dataset_rgb_salt_and_pepper_25/assatigue/original/k5/advanced_stats",
    "n_estimators": 50,
    "test_size": 0.2,
    "random_state": 42,
    "cv_folds": 5
  },
  "dataset_info": {
    "data_directory": "/home/brusc/Projects/random_forest/datasets/dataset_rgb_salt_and_pepper_25/original",
    "area_name": "assatigue",
    "total_images": 116,
    "classes": {
      "low_veg": 40,
      "water": 40,
      "trees": 36
    },
    "image_shape": [
      3,
      128,
      128
    ],
    "total_features_available": 54,
    "feature_method": "advanced_stats",
    "dataset_type": "original",
    "k_features": 5
  },
  "feature_selection": {
    "method": "SelectKBest_k5",
    "num_features": 5,
    "selected_features": [
      "R_skew",
      "R_cv",
      "R_iqr",
      "B_cv",
      "B_p50"
    ],
    "feature_scores": [
      0.6788243146583359,
      0.6728144002432495,
      0.674566264767797,
      0.7538827810961133,
      0.7102558381157222
    ]
  },
  "performance": {
    "test_accuracy": 0.9583333333333334,
    "cv_mean_accuracy": 0.9826086956521738,
    "cv_std_accuracy": 0.034782608695652195,
    "cv_scores": [
      1.0,
      1.0,
      0.9130434782608695,
      1.0,
      1.0
    ],
    "classification_report": {
      "low_veg": {
        "precision": 0.8888888888888888,
        "recall": 1.0,
        "f1-score": 0.9411764705882353,
        "support": 8.0
      },
      "trees": {
        "precision": 1.0,
        "recall": 1.0,
        "f1-score": 1.0,
        "support": 8.0
      },
      "water": {
        "precision": 1.0,
        "recall": 0.875,
        "f1-score": 0.9333333333333333,
        "support": 8.0
      },
      "accuracy": 0.9583333333333334,
      "macro avg": {
        "precision": 0.9629629629629629,
        "recall": 0.9583333333333334,
        "f1-score": 0.9581699346405229,
        "support": 24.0
      },
      "weighted avg": {
        "precision": 0.9629629629629629,
        "recall": 0.9583333333333334,
        "f1-score": 0.9581699346405229,
        "support": 24.0
      }
    },
    "confusion_matrix": [
      [
        8,
        0,
        0
      ],
      [
        0,
        8,
        0
      ],
      [
        1,
        0,
        7
      ]
    ]
  },
  "model_files": {
    "trained_model": "trained_model.joblib",
    "scaler": "scaler.joblib",
    "feature_selector": "feature_selector.joblib",
    "feature_names": "feature_names.json"
  },
  "timestamp": "2025-07-22T16:05:36.271448"
}